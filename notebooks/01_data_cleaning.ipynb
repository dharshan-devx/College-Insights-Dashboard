{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4e8448",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base (Python 3.13.5)' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages."
     ]
    }
   ],
   "source": [
    "# Create a file with Jupyter Notebook content.\n",
    "jupyter_notebook_content = \"\"\"\n",
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# College Insights Dashboard: Data Cleaning & Preparation (01_data_cleaning.ipynb)\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook serves as the initial step of the data science project. The primary goal here is to load the raw data from the CSV files, perform a thorough cleaning and inspection, and prepare a single, clean DataFrame for subsequent analysis and modeling. This is a critical step to ensure the integrity of our insights.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Setup and Library Imports\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"import logging\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set up logging for informative output\\n\",\n",
    "    \"logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set the path to the data directory\\n\",\n",
    "    \"data_path = '../data/'\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Load Raw Datasets\\n\",\n",
    "    \"\\n\",\n",
    "    \"We'll load each of the four CSV files into separate pandas DataFrames to inspect their individual structures.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"try:\\n\",\n",
    "    \"    students_df = pd.read_csv(os.path.join(data_path, 'students.csv'))\\n\",\n",
    "    \"    subjects_df = pd.read_csv(os.path.join(data_path, 'subjects.csv'))\\n\",\n",
    "    \"    marks_df = pd.read_csv(os.path.join(data_path, 'marks.csv'))\\n\",\n",
    "    \"    attendance_df = pd.read_csv(os.path.join(data_path, 'attendance.csv'))\\n\",\n",
    "    \"    logging.info(\\\"All raw data files loaded successfully.\\\")\\n\",\n",
    "    \"except FileNotFoundError as e:\\n\",\n",
    "    \"    logging.error(f\\\"Error loading data: {e}. Please ensure the CSV files are in the '{data_path}' directory.\\\")\\n\",\n",
    "    \"    # Exit if files are not found\\n\",\n",
    "    \"    raise\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Initial Data Inspection\\n\",\n",
    "    \"\\n\",\n",
    "    \"A quick look at the head and info of each DataFrame helps us understand its structure, data types, and potential issues like missing values.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"logging.info(\\\"\\\\n--- students_df info ---\\\")\\n\",\n",
    "    \"students_df.info()\\n\",\n",
    "    \"logging.info(\\\"\\\\n--- subjects_df info ---\\\")\\n\",\n",
    "    \"subjects_df.info()\\n\",\n",
    "    \"logging.info(\\\"\\\\n--- marks_df info ---\\\")\\n\",\n",
    "    \"marks_df.info()\\n\",\n",
    "    \"logging.info(\\\"\\\\n--- attendance_df info ---\\\")\\n\",\n",
    "    \"attendance_df.info()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. Merging Datasets\\n\",\n",
    "    \"\\n\",\n",
    "    \"To perform meaningful analysis, we need to combine all the information into a single DataFrame. We will use a series of `pd.merge()` calls, starting with the `marks` DataFrame as our base.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Merge marks with student data\\n\",\n",
    "    \"combined_df = marks_df.merge(students_df, on='student_id', how='left')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Merge the result with subject data\\n\",\n",
    "    \"combined_df = combined_df.merge(subjects_df, on='subject_id', how='left')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Finally, merge with attendance data\\n\",\n",
    "    \"combined_df = combined_df.merge(attendance_df, on=['student_id', 'subject_id'], how='left')\\n\",\n",
    "    \"\\n\",\n",
    "    \"logging.info(\\\"Datasets merged successfully.\\\")\\n\",\n",
    "    \"combined_df.head()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5. Data Cleaning & Feature Engineering\\n\",\n",
    "    \"\\n\",\n",
    "    \"Now that the data is merged, we'll perform final cleaning steps and create new features that will be useful for our analysis and modeling.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# 1. Handle Missing Values: Drop any rows that may have missing data after the merge (e.g., if a student or subject ID was not found)\\n\",\n",
    "    \"logging.info(f\\\"Shape before dropping NaNs: {combined_df.shape}\\\")\\n\",\n",
    "    \"combined_df.dropna(inplace=True)\\n\",\n",
    "    \"logging.info(f\\\"Shape after dropping NaNs: {combined_df.shape}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# 2. Rename columns for clarity\\n\",\n",
    "    \"combined_df.rename(columns={\\n\",\n",
    "    \"    'name_x': 'student_name',\\n\",\n",
    "    \"    'name_y': 'subject_name',\\n\",\n",
    "    \"    'department': 'department',\\n\",\n",
    "    \"    'marks': 'marks',\\n\",\n",
    "    \"    'attendance_percentage': 'attendance'\\n\",\n",
    "    \"}, inplace=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# 3. Feature Engineering: Create a 'pass_status' column (Pass if marks >= 40, Fail otherwise)\\n\",\n",
    "    \"combined_df['pass_status'] = combined_df['marks'].apply(lambda x: 'Pass' if x >= 40 else 'Fail')\\n\",\n",
    "    \"\\n\",\n",
    "    \"logging.info(\\\"Columns renamed and 'pass_status' feature engineered.\\\")\\n\",\n",
    "    \"combined_df.head()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 6. Final Data Inspection and Summary\\n\",\n",
    "    \"\\n\",\n",
    "    \"A final check to ensure all our data is in the correct format before we proceed to the next steps.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"combined_df.info()\\n\",\n",
    "    \"logging.info(\\\"\\\\n--- Summary of Final DataFrame ---\\\")\\n\",\n",
    "    \"logging.info(f\\\"Number of records: {len(combined_df)}\\\")\\n\",\n",
    "    \"logging.info(f\\\"Number of unique students: {combined_df['student_id'].nunique()}\\\")\\n\",\n",
    "    \"logging.info(f\\\"Number of unique subjects: {combined_df['subject_id'].nunique()}\\\")\\n\",\n",
    "    \"logging.info(f\\\"Final DataFrame ready for analysis.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": []\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.10.12\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "with open('notebooks/01_data_cleaning.ipynb', 'w') as f:\n",
    "    f.write(jupyter_notebook_content)\n",
    "print('The file \"notebooks/01_data_cleaning.ipynb\" has been created.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
